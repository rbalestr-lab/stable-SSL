hydra:
  run:
    dir: ${trainer.logger.base_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${trainer.logger.base_dir}/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    timeout_min: 1000
    cpus_per_task: ${trainer.hardware.cpus_per_task}
    gpus_per_node: ${trainer.hardware.world_size}
    tasks_per_node: ${trainer.hardware.world_size}
    mem_gb: null
    nodes: 1
    name: ${hydra.job.name}
    stderr_to_stdout: false
    _target_: hydra_plugins.hydra_submitit_launcher.submitit_launcher.SlurmLauncher
    partition: gpu
    qos: null
    comment: null
    constraint: null
    exclude: null
    gres: null
    cpus_per_gpu: null
    gpus_per_task: null
    mem_per_gpu: null
    mem_per_cpu: null
    account: null
    signal_delay_s: 120
    max_num_timeout: 5
    additional_parameters: {}
    array_parallelism: 256
    setup: null
  sweeper:
    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper
    max_batch_size: null
    params: null
  help:
    app_name: ${hydra.job.name}
    header: '${hydra.help.app_name} is powered by Hydra.

      '
    footer: 'Powered by Hydra (https://hydra.cc)

      Use --hydra-help to view Hydra specific help

      '
    template: '${hydra.help.header}

      == Configuration groups ==

      Compose your configuration from those groups (group=option)


      $APP_CONFIG_GROUPS


      == Config ==

      Override anything in the config (foo.bar=value)


      $CONFIG


      ${hydra.help.footer}

      '
  hydra_help:
    template: 'Hydra (${hydra.runtime.version})

      See https://hydra.cc for more info.


      == Flags ==

      $FLAGS_HELP


      == Configuration groups ==

      Compose your configuration from those groups (For example, append hydra/job_logging=disabled
      to command line)


      $HYDRA_CONFIG_GROUPS


      Use ''--cfg hydra'' to Show the Hydra config.

      '
    hydra_help: ???
  hydra_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][HYDRA] %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
    root:
      level: INFO
      handlers:
      - console
    loggers:
      logging_example:
        level: DEBUG
    disable_existing_loggers: false
  job_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.runtime.output_dir}/${hydra.job.name}.log
    root:
      level: INFO
      handlers:
      - console
      - file
    disable_existing_loggers: false
  env: {}
  mode: MULTIRUN
  searchpath: []
  callbacks: {}
  output_subdir: .hydra
  overrides:
    hydra:
    - hydra.mode=MULTIRUN
    task: []
  job:
    name: run
    chdir: false
    override_dirname: ''
    id: ???
    num: ???
    config_name: global_config
    env_set: {}
    env_copy: []
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: []
  runtime:
    version: 1.3.2
    version_base: '1.2'
    cwd: /oscar/home/hvanasse/stable-SSL/benchmarks
    config_sources:
    - path: hydra.conf
      schema: pkg
      provider: hydra
    - path: /oscar/home/hvanasse/stable-SSL/benchmarks/config
      schema: file
      provider: main
    - path: ''
      schema: structured
      provider: schema
    output_dir: ???
    choices:
      hydra/env: default
      hydra/callbacks: null
      hydra/job_logging: default
      hydra/hydra_logging: default
      hydra/hydra_help: default
      hydra/help: default
      hydra/sweeper: basic
      hydra/launcher: submitit_slurm
      hydra/output: default
  verbose: false
trainer:
  data:
    _num_classes: 10
    _num_samples: 50000
    base:
      _target_: torch.utils.data.DataLoader
      batch_size: 256
      drop_last: true
      shuffle: true
      num_workers: ${trainer.hardware.cpus_per_task}
      dataset:
        _target_: torchvision.datasets.CIFAR10
        root: ~/data
        train: true
        download: true
        transform:
          _target_: stable_ssl.data.MultiViewSampler
          transforms:
          - _target_: torchvision.transforms.v2.Compose
            transforms:
            - _target_: torchvision.transforms.v2.RandomResizedCrop
              size: 32
              scale:
              - 0.2
              - 1.0
            - _target_: torchvision.transforms.v2.RandomHorizontalFlip
              p: 0.5
            - _target_: torchvision.transforms.v2.ColorJitter
              brightness: 0.4
              contrast: 0.4
              saturation: 0.2
              hue: 0.1
            - _target_: torchvision.transforms.v2.RandomGrayscale
              p: 0.2
            - _target_: torchvision.transforms.v2.ToImage
            - _target_: torchvision.transforms.v2.ToDtype
              dtype:
                _target_: stable_ssl.utils.str_to_dtype
                _args_:
                - float32
              scale: true
          - ${trainer.data.base.dataset.transform.transforms.0}
    test_out:
      _target_: torch.utils.data.DataLoader
      batch_size: 256
      num_workers: ${trainer.hardware.cpus_per_task}
      dataset:
        _target_: torchvision.datasets.CIFAR10
        train: false
        root: ~/data
        transform:
          _target_: torchvision.transforms.v2.Compose
          transforms:
          - _target_: torchvision.transforms.v2.ToImage
          - _target_: torchvision.transforms.v2.ToDtype
            dtype:
              _target_: stable_ssl.utils.str_to_dtype
              _args_:
              - float32
            scale: true
  networks:
    backbone:
      _target_: stable_ssl.utils.load_backbone
      name: resnet18
      dataset: CIFAR10
    projector:
      _target_: torch.nn.Sequential
      _args_:
      - _target_: torch.nn.Linear
        in_features: 512
        out_features: 2048
        bias: false
      - _target_: torch.nn.BatchNorm1d
        num_features: ${trainer.networks.projector._args_.0.out_features}
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Linear
        in_features: ${trainer.networks.projector._args_.0.out_features}
        out_features: 128
        bias: false
    projector_classifier:
      _target_: torch.nn.Linear
      in_features: 128
      out_features: ${trainer.data._num_classes}
    backbone_classifier:
      _target_: torch.nn.Linear
      in_features: 512
      out_features: ${trainer.data._num_classes}
  optim:
    epochs: 1000
    max_steps: 1000
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 0.01
      weight_decay: 1.0e-06
    scheduler:
      _target_: torch.optim.lr_scheduler.OneCycleLR
      _partial_: true
      max_lr: 0.01
      epochs: ${trainer.optim.epochs}
      steps_per_epoch: ${eval:'${trainer.data._num_samples} // ${trainer.data.${trainer.train_on}.batch_size}'}
  logger:
    metrics:
      base:
        acc1:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${trainer.data._num_classes}
          top_k: 1
        acc5:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${trainer.data._num_classes}
          top_k: 5
      test_out:
        acc1:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${trainer.data._num_classes}
          top_k: 1
        acc5:
          _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${trainer.data._num_classes}
          top_k: 5
    base_dir: ./
    level: 20
    checkpoint_frequency: 1
    every_step: 1
  _target_: stable_ssl.JointEmbedding
  objective:
    _target_: stable_ssl.NTXEntLoss
    temperature: 0.5
  eval_only: false
  train_on: base
  hardware:
    seed: 0
    float16: true
    device: cuda:0
    world_size: 1
    cpus_per_task: 6
