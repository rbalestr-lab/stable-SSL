# reproduces the training setup of the paper:
# 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'
# https://arxiv.org/abs/1706.02677


defaults:
  - override hydra/launcher: submitit_slurm

hydra:
  launcher:
    tasks_per_node: ${hardware.world_size}
    gpus_per_node: ${hardware.world_size}
    partition: gpu
    cpus_per_task: 4

data:
  train_on: base
  base:
    name: ImageNet
    path: /sss/jobtmp/hvanasse/ImageNet
    batch_size: 256
    split: train
    drop_last: True
    shuffle: True
    num_workers: -1
    transforms:
      view1:
      - name: ToTensor
      - name: Normalize
        kwargs:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - name: RandomResizedCrop
        kwargs:
          size: 224
          scale:
            - 0.08
            - 1.0
      - name: RandomHorizontalFlip
        kwargs:
          p: 0.5
  test_out:
    name: ImageNet
    path: /sss/jobtmp/hvanasse/ImageNet
    batch_size: 256
    split: val
    drop_last: False
    num_workers: -1
    transforms:
      view1:
      - name: ToTensor
      - name: Normalize
        kwargs:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - name: Resize
        kwargs:
          size: 256
      - name: CenterCrop
        kwargs:
          size: 224

model:
  name: Supervised
  backbone_model: resnet50

optim:
  optimizer: SGD
  epochs: 90
  lr: 0.256
  weight_decay: 3.0517578125e-05

hardware:
  seed: 0
  float16: true
  gpu_id: 0
  world_size: 2
